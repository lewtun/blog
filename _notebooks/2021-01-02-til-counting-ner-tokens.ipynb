{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating named entity frequencies\n",
    "> Munging data with ðŸ¤—  Datasets\n",
    "\n",
    "- comments: false\n",
    "- categories: [til,nlp,huggingface]\n",
    "- badges: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# uncomment if running on Colab\n",
    "# !pip install datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.1 1.1.3\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import warnings\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "datasets.logging.set_verbosity_error()\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(transformers.__version__, datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For named entity recognition tasks, a handy measure of class imbalance is to calculate the frequency of named entities in the data. I wanted to do this with the `datasets` library for documents annotated in the [\"inside-outside-beginning\" (IOB2) format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging). \n",
    "\n",
    "One problem I encountered was that `datasets` tends to represent the entities in terms of _label IDs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'id': '0',\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "conll = load_dataset(\"conll2003\")\n",
    "conll['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I created a simple function that makes use of the `Dataset.features` attribute and `ClassLabel.int2str` method to perform the mapping from ID to human-readable string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'id': '0',\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'ner_tags_str': ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def create_tag_names(ds: Dataset, tags_col: str) -> Dataset:\n",
    "    # pick out the ClassLabel feature from feature\n",
    "    tags = ds[\"train\"].features[tags_col].feature\n",
    "    # apply the ClassLabel.int2str method to each token\n",
    "    proc_fn = lambda x : {f\"{tags_col}_str\": [tags.int2str(idx) for idx in x[tags_col]]}\n",
    "    return ds.map(proc_fn)\n",
    "\n",
    "\n",
    "conll = create_tag_names(conll, 'ner_tags')\n",
    "conll['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some help from my [partner-in-crime](https://twitter.com/lvwerra?s=20), the final step was to iterate over each example, collect all the _B-_ tags in a list (since the _I-_ tags refer to the same entity), and then use a bit of `chain` magic to flatten the list of lists per split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORG</th>\n",
       "      <th>MISC</th>\n",
       "      <th>PER</th>\n",
       "      <th>LOC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6321</td>\n",
       "      <td>3438</td>\n",
       "      <td>6600</td>\n",
       "      <td>7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>1341</td>\n",
       "      <td>922</td>\n",
       "      <td>1842</td>\n",
       "      <td>1837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>1661</td>\n",
       "      <td>702</td>\n",
       "      <td>1617</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ORG  MISC   PER   LOC\n",
       "train       6321  3438  6600  7140\n",
       "validation  1341   922  1842  1837\n",
       "test        1661   702  1617  1668"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_tag_frequencies(ds: Dataset, tags_col: str) -> pd.DataFrame:\n",
    "    split2freqs = {}\n",
    "\n",
    "    for split in ds.keys():\n",
    "        tag_names = []\n",
    "        for row in ds[split][tags_col]:\n",
    "            tag_names.append([tag.split('-')[1] for tag in row if tag.startswith(\"B\")])\n",
    "            # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n",
    "        split2freqs[split] = Counter(chain.from_iterable(tag_names))\n",
    "\n",
    "    return pd.DataFrame.from_dict(split2freqs, orient=\"index\")\n",
    "\n",
    "calculate_tag_frequencies(conll, 'ner_tags_str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's compare with Table 2 from the [CoNLL-2003 paper](https://www.aclweb.org/anthology/W03-0419.pdf): \n",
    "\n",
    "![](my_icons/conll.png)\n",
    "\n",
    "It works!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
