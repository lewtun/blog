{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing PyTorch Datasets\n",
    "\n",
    "- comments: false\n",
    "- categories: [til,nlp,pytorch]\n",
    "- badges: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# uncomment if running on Colab\n",
    "# !pip install transformers datasets numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import warnings\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "datasets.logging.set_verbosity_error()\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to run some experiments with [Victor Sanh's](https://twitter.com/SanhEstPasMoi?s=20) implementation of [movement pruning](https://github.com/huggingface/transformers/tree/master/examples/research_projects/movement-pruning) so that I could compare against a custom `Trainer` I had implemented. Since each epoch of training on SQuAD takes around 2 hours on a single GPU, I wanted to speed-up the comparison by prune-tuning on a _subset_ of the data.\n",
    "\n",
    "Since it's been a while that I've worked directly with PyTorch `Dataset` objects,{% fn 1 %} I'd forgotten that one can't use a naive slicing of the dataset. For example, the following will fail:\n",
    "\n",
    "```python\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "train_ds = ...\n",
    "sample_ds = train_ds[:10] # folly!\n",
    "sample_sampler = RandomSampler(sample_ds)\n",
    "sample_dl = DataLoader(sample_ds, sampler=sample_sampler, batch_size=4)\n",
    "next(iter(sample_dl)) # KeyError or similar :(\n",
    "```\n",
    "\n",
    "The reason this occurs is because slicing `train_ds` will return an object of a different _type_ to `Dataset` (e.g. a `dict`), so the `RandomSampler` doesn't know how to produce appropriate samples for the `DataLoader`.\n",
    "\n",
    "The solution I ended up with is to use the `Subset` class to create the desired subset:\n",
    "\n",
    "```python\n",
    "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
    "\n",
    "train_ds = ...\n",
    "num_train_samples = 100\n",
    "sample_ds = Subset(train_dataset, np.arange(num_train_samples))\n",
    "sample_sampler = RandomSampler(sample_ds)\n",
    "sample_dl = DataLoader(sample_ds, sampler=sample_sampler, batch_size=4)\n",
    "next(iter(sample_dl))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this in action, we'll use the IMDB dataset as an example. First let's download and unpack the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data\n",
    "!tar -xf data/aclImdb_v1.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the `transformers` [docs](https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews), the next thing we need is to read the samples and labels. The following code does the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.',\n",
       " 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA = Path('data/aclImdb')\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir == \"neg\" else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = read_imdb_split(f'{DATA}/train')\n",
    "# peek at first sample and label\n",
    "train_texts[0], train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tokenize the texts, which can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can define a custom `Dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_ds = IMDbDataset(train_encodings, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of `train_ds` is a `dict` with keys corresponding to the inputs expected in the `forward` pass of a Transformer model like BERT. If we take a slice, then we get tensors for each of the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2005,  1037,  ...,     0,     0,     0],\n",
       "         [  101, 13576,  5469,  ...,     0,     0,     0],\n",
       "         [  101,  1037,  5024,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2023,  2001,  ...,     0,     0,     0],\n",
       "         [  101,  2081,  2044,  ...,  3286,  1011,   102],\n",
       "         [  101,  2005,  1037,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `dict` type is not suitable for sampling from, so the solution is to wrap our `Dataset` with `Subset` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "num_train_examples = 100\n",
    "sample_ds = Subset(train_ds, np.arange(num_train_examples))\n",
    "assert len(sample_ds) == num_train_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's compare the raw text against the decoded examples in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan \" the skipper \" hale jr. as a police sgt.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample_ds[0]['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, how about the last example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful film, pure cassavetes style. gena rowland gives a stunning performance of a declining actress, dealing with success, aging, loneliness... and alcoholism. she tries to escape her own subconscious ghosts, embodied by the death spectre of a young girl. acceptance of oneself, of human condition, though its overall difficulties, is the real purpose of the film. the parallel between the theatrical sequences and the film itself are puzzling : it's like if the stage became a way out for the heroin. if all american movies could only be that top - quality, dealing with human relations on an adult level, not trying to infantilize and standardize feelings... one of the best dramas ever. 10 / 10. \n",
      "\n",
      "Beautiful film, pure Cassavetes style. Gena Rowland gives a stunning performance of a declining actress, dealing with success, aging, loneliness...and alcoholism. She tries to escape her own subconscious ghosts, embodied by the death spectre of a young girl. Acceptance of oneself, of human condition, though its overall difficulties, is the real purpose of the film. The parallel between the theatrical sequences and the film itself are puzzling: it's like if the stage became a way out for the Heroin. If all american movies could only be that top-quality, dealing with human relations on an adult level, not trying to infantilize and standardize feelings... One of the best dramas ever. 10/10.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(sample_ds[-1]['input_ids'], skip_special_tokens=True), \"\\n\")\n",
    "print(train_texts[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to define the sampler and dataloader and we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 13576,  5469,  ...,     0,     0,     0],\n",
       "         [  101,  1037,  5024,  ...,     0,     0,     0],\n",
       "         [  101,  2005,  1037,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 1, 1])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "sample_sampler = RandomSampler(sample_ds)\n",
    "sample_dl = DataLoader(sample_ds, sampler=train_sampler, batch_size=4)\n",
    "next(iter(sample_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "## Footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ \"Mostly because I've been corrupted by the `datasets` and `fastai` APIs\" | fndetail: 1 }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
