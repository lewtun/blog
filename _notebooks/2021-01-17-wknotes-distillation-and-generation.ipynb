{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Weeknotes: Distilling distilled transformers\"\n",
    "\n",
    "- comments: false\n",
    "- categories: [weeknotes,nlp,huggingface,transformers]\n",
    "- badges: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week I mostly worked on getting my knowledge distillation code up and running, doing some pair-programming with [Leandro von Werra](https://twitter.com/lvwerra) to re-implement Google's [_Unsupervised Data Augmentation for Consistency Training_](https://arxiv.org/abs/1904.12848), and reviewing a book chapter on decoding strategies for text generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $(\\mathrm{DistilBERT})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I extended my question answering [analysis](https://github.com/lewtun/transformerlab/tree/master) with `transformers` to implement a proof-of-concept for _task-specific_ knowledge distillation.{% fn 1 %} Unlike _task-agnostic_ distillation where the transfer of knowledge from teacher to student is done during _pretraining_, the task-specific approach involves using a teacher to augment the cross-entropy loss of the student during _finetuning_:\n",
    "\n",
    "$${\\cal L}(\\mathbf{x}|T) = - \\sum_i \\bar{y}_i\\log y_i(\\mathbf{x}|T) -T^2 \\sum_i \\hat{y}_i(\\mathbf{x}|T)\\log y_i(\\mathbf{x}|T)$$\n",
    "\n",
    "Here $T$ is the temperature, $\\hat{y}$ are the outputs from the model, $\\bar{y}$ the ground-truth labels, and $y_i$ a softmax with temperature.\n",
    "\n",
    "This neat idea comes from the [DistilBERT paper](https://arxiv.org/pdf/1910.01108.pdf), where the authors found that including a \"second step of distillation\" produced a student that performed better than simply finetuning the distilled language model:\n",
    "\n",
    "> We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of the two approaches is shown in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"distillation\" caption=\"Task-specific distillation (left) versus task-agnostic distillation (right). Figure from FastFormers by Y. Kim and H. Awadalla [arXiv:2010.13382].\" src=\"my_icons/distillation.png\" id=\"distillation\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find this idea to be quite appealing for deploying Transformers in production environments as we get the benefits of speed from using a distilled language model, yet largely preserve the performance of the teacher. \n",
    "\n",
    "So my task this week was to reproduce the SQuAD v1.1 results from Table 2 of the DistilBERT paper. To do this I integrated [Sylvain Gugger's](https://twitter.com/GuggerSylvain?s=20) question answering material (see [last weeknotes](https://lewtun.github.io/blog/weeknotes/nlp/huggingface/transformers/2021/01/10/wknotes-question-answering.html)) together with [Victor Sanh's](https://twitter.com/SanhEstPasMoi?s=20) [implementation](`https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation`) of knowledge distillation.{% fn 2 %}\n",
    "\n",
    "The main bit of work was to create a `Trainer` class that could:\n",
    "\n",
    "* handle two models at once, i.e. for the teacher and student\n",
    "* run evaluation during training to get feedback on the distillation process\n",
    "\n",
    "The solution I ended up with involved subclassing the `QuestionAnsweringTrainer` I had previously adapted from Sylvain and simply overriding the `compute_loss` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DistillationTrainer(QuestionAnsweringTrainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        ...\n",
    "        \n",
    "    def compute_loss(self, model, inputs):\n",
    "        inputs_stu = {...}\n",
    "        outputs_stu = model(**inputs_stu)\n",
    "        loss = outputs_stu.loss\n",
    "        start_logits_stu = outputs_stu.start_logits\n",
    "        end_logits_stu = outputs_stu.end_logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher(**inputs)\n",
    "            start_logits_tea = outputs_tea.start_logits\n",
    "            end_logits_tea = outputs_tea.end_logits\n",
    "        \n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_start = (\n",
    "            loss_fct(\n",
    "                F.log_softmax(start_logits_stu / self.args.temperature, dim=-1),\n",
    "                F.softmax(start_logits_tea / self.args.temperature, dim=-1))\n",
    "            * (self.args.temperature ** 2))\n",
    "        loss_end = (\n",
    "            loss_fct(\n",
    "                F.log_softmax(end_logits_stu / self.args.temperature, dim=-1),\n",
    "                F.softmax(end_logits_tea / self.args.temperature, dim=-1))\n",
    "            * (self.args.temperature ** 2))\n",
    "        loss_ce = (loss_start + loss_end) / 2.0\n",
    "        loss = self.args.alpha_ce * loss_ce + self.args.alpha_squad * loss\n",
    "        return loss\n",
    "```\n",
    "\n",
    "By using DistilBERT-base as the student and BERT-base fine-tuned on SQuAD v1.1 as the teacher, I was able to get within spitting distance of the published results (Exact Match/F1 = 79.1/86.9), with the differences likely due to the choice of hyperparameters:\n",
    "\n",
    "<img alt=\"distillation\" width=500 caption=\"Evaluation metrics on SQuAD v1.1 for task-specific distillation\" src=\"my_icons/distillation-results.png\" id=\"distillation\"/>\n",
    "\n",
    "Overall, I'm pretty happy with how this turned out and am starting to appreciate the power of the \"trainer paradigm\", where one can abstract away tons of boilerplate (and error-prone) code for the training loop, evaluation, prediction etc and just focus on overriding the parts you need. I'm looking forward to pushing this analysis one step further with pruning and quantization - that's on the menu for next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers this week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week I've been reading up on OpenAI's GPT papers to better understand how decoding methods for text generation work with conditional language models:\n",
    "\n",
    "* [_Language Models are Unsupervised Multitask Learners_](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever (2019)\n",
    "* [_Language Models are Few-Shot Learners_](https://arxiv.org/abs/2005.14165) by Tom B. Brown et al. (2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIL this week\n",
    "\n",
    "* [Recovering columns hidden by the ðŸ¤— Trainer](https://lewtun.github.io/blog/til/nlp/huggingface/transformers/2021/01/15/til-recovering-hidden-trainer-columns.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "## Footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ 'As far as I know, this term was coined in the [_FastFormers: Highly Efficient Transformer Models for Natural Language Understanding_](https://arxiv.org/abs/2010.13382) paper by Y. Kim and H. Awadalla in their' | fndetail: 1 }}\n",
    "\n",
    "{{ 'Thanks to [Thomas Wolf](https://twitter.com/Thom_Wolf?s=20) for pointing me to this resource.' | fndetail: 2 }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
