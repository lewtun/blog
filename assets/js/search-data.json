{
  
    
        "post0": {
            "title": "Calculating named entity frequencies",
            "content": "For named entity recognition tasks, a handy measure of class imbalance is to calculate the frequency of named entities in the data. I wanted to do this with the datasets library for documents annotated in the &quot;inside-outside-beginning&quot; (IOB2) format. . One problem I encountered was that datasets tends to represent the entities in terms of label IDs . from datasets import load_dataset conll = load_dataset(&quot;conll2003&quot;) conll[&#39;train&#39;][0] . {&#39;chunk_tags&#39;: [11, 21, 11, 12, 21, 22, 11, 12, 0], &#39;id&#39;: &#39;0&#39;, &#39;ner_tags&#39;: [3, 0, 7, 0, 0, 0, 7, 0, 0], &#39;pos_tags&#39;: [22, 42, 16, 21, 35, 37, 16, 21, 7], &#39;tokens&#39;: [&#39;EU&#39;, &#39;rejects&#39;, &#39;German&#39;, &#39;call&#39;, &#39;to&#39;, &#39;boycott&#39;, &#39;British&#39;, &#39;lamb&#39;, &#39;.&#39;]} . so I created a simple function that makes use of the Dataset.features attribute and ClassLabel.int2str method to perform the mapping from ID to human-readable string: . from datasets import Dataset def create_tag_names(ds: Dataset, tags_col: str) -&gt; Dataset: # pick out the ClassLabel feature from feature tags = ds[&quot;train&quot;].features[tags_col].feature # apply the ClassLabel.int2str method to each token proc_fn = lambda x : {f&quot;{tags_col}_str&quot;: [tags.int2str(idx) for idx in x[tags_col]]} return ds.map(proc_fn) . conll = create_tag_names(conll, &#39;ner_tags&#39;) conll[&#39;train&#39;][0] . {&#39;chunk_tags&#39;: [11, 21, 11, 12, 21, 22, 11, 12, 0], &#39;id&#39;: &#39;0&#39;, &#39;ner_tags&#39;: [3, 0, 7, 0, 0, 0, 7, 0, 0], &#39;ner_tags_str&#39;: [&#39;B-ORG&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;O&#39;, &#39;O&#39;], &#39;pos_tags&#39;: [22, 42, 16, 21, 35, 37, 16, 21, 7], &#39;tokens&#39;: [&#39;EU&#39;, &#39;rejects&#39;, &#39;German&#39;, &#39;call&#39;, &#39;to&#39;, &#39;boycott&#39;, &#39;British&#39;, &#39;lamb&#39;, &#39;.&#39;]} . With some help from my partner-in-crime, the final step was to iterate over each example, collect all the B- tags in a list (since the I- tags refer to the same entity), and then use a bit of chain magic to flatten the list of lists per split: . import pandas as pd from itertools import chain from collections import Counter def calculate_tag_frequencies(ds: Dataset, tags_col: str) -&gt; pd.DataFrame: split2freqs = {} for split in ds.keys(): tag_names = [] for row in ds[split][tags_col]: tag_names.append([tag.split(&#39;-&#39;)[1] for tag in row if tag.startswith(&quot;B&quot;)]) # chain.from_iterable([&#39;ABC&#39;, &#39;DEF&#39;]) --&gt; A B C D E F split2freqs[split] = Counter(chain.from_iterable(tag_names)) return pd.DataFrame.from_dict(split2freqs, orient=&quot;index&quot;) calculate_tag_frequencies(conll, &#39;ner_tags_str&#39;) . ORG MISC PER LOC . train 6321 | 3438 | 6600 | 7140 | . validation 1341 | 922 | 1842 | 1837 | . test 1661 | 702 | 1617 | 1668 | . As a sanity check, let&#39;s compare with Table 2 from the CoNLL-2003 paper: . . It works! .",
            "url": "https://lewtun.github.io/blog/til/nlp/huggingface/2021/01/02/til-counting-ner-tokens.html",
            "relUrl": "/til/nlp/huggingface/2021/01/02/til-counting-ner-tokens.html",
            "date": " • Jan 2, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Using data collators for training and error analysis",
            "content": "Recently, Sylvain Gugger from HuggingFace has created some nice tutorials on using transformers for text classification and named entity recognition. One trick that caught my attention was the use of a data collator in the trainer, which automatically pads the model inputs in a batch to the length of the longest example. This bypasses the need to set a global maximum sequence length, and in practice leads to faster training since we perform fewer redundant computations on the padded tokens and attention masks. . I wanted to use a data collator for both training and error analysis (e.g. by inspecting the top losses of the model). One problem: during training, each batch is collated on the fly so how do I pad my inputs in subsequent Dataset.map operations? . The solution I ended up with was to simply grab the data collator from the trainer and use it in my post-processing functions: . data_collator = trainer.data_collator def processing_function(batch): # pad inputs and (possibly) labels batch = data_collator(batch) ... return batch . For an end-to-end example, let&#39;s grab 1,000 examples from the IMDB dataset: . from datasets import load_dataset imdb = (load_dataset(&#39;imdb&#39;, split=&#39;train&#39;) .train_test_split(train_size=800, test_size=200) ) imdb . DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 800 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 200 }) }) . Next, let&#39;s load a pretrained model and its corresponding tokenizer: . import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification num_labels = 2 model_name = &#39;distilbert-base-cased&#39; device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) tokenizer = AutoTokenizer.from_pretrained(model_name) model = (AutoModelForSequenceClassification .from_pretrained(model_name, num_labels=num_labels) .to(device) ) . Before fine-tuning the model, we need to tokenize and encode the dataset, so let&#39;s do that with a simple Dataset.map operation: . def tokenize_and_encode(batch): return tokenizer(batch[&#39;text&#39;], truncation=True) imdb_enc = imdb.map(tokenize_and_encode, batched=True) imdb_enc . DatasetDict({ train: Dataset({ features: [&#39;attention_mask&#39;, &#39;input_ids&#39;, &#39;label&#39;, &#39;text&#39;], num_rows: 800 }) test: Dataset({ features: [&#39;attention_mask&#39;, &#39;input_ids&#39;, &#39;label&#39;, &#39;text&#39;], num_rows: 200 }) }) . The final step is to define the metrics . import numpy as np from datasets import load_metric accuracy_score = load_metric(&quot;accuracy&quot;) def compute_metrics(eval_pred): predictions, labels = eval_pred predictions = np.argmax(predictions, axis=1) return accuracy_score.compute(predictions=predictions, references=labels) . the arguments for the trainer . from transformers import TrainingArguments batch_size = 16 logging_steps = len(imdb_enc[&#39;train&#39;]) // batch_size training_args = TrainingArguments( output_dir=&quot;results&quot;, num_train_epochs=1, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, evaluation_strategy=&quot;epoch&quot;, disable_tqdm=False, logging_steps=logging_steps ) . and the trainer itself: . . Important: The trainer will remove in-place any dataset columns of str type, so in this example imdb_enc loses the text column. . from transformers import Trainer trainer = Trainer( model=model, args=training_args, compute_metrics=compute_metrics, train_dataset=imdb_enc[&#39;train&#39;], eval_dataset=imdb_enc[&#39;test&#39;], tokenizer=tokenizer ) trainer.train(); . . [50/50 00:32, Epoch 1/1] Epoch Training Loss Validation Loss Accuracy . 1 | 0.390015 | 0.328747 | 0.875000 | . By default, the Trainer class uses the simple default_data_collator to collate batches of dict-like objects, but by passing the tokenizer we get a DataCollatorWithPadding instead: . data_collator = trainer.data_collator type(data_collator) . transformers.data.data_collator.DataCollatorWithPadding . To see how this collator works, let&#39;s pass a dummy batch and observe that both the input_ids and attention_mask are padded as expected: . batch = {&#39;input_ids&#39;: [[0,1,2], [0,1,2,3,4,5]]} data_collator(batch) . {&#39;input_ids&#39;: tensor([[0, 1, 2, 0, 0, 0], [0, 1, 2, 3, 4, 5]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1]])} . Finally, we can calculate the loss per example with the following function:1 . def loss_per_example(batch): batch = data_collator(batch) input_ids = torch.tensor(batch[&quot;input_ids&quot;], device=device) attention_mask = torch.tensor(batch[&quot;attention_mask&quot;], device=device) labels = torch.tensor(batch[&quot;labels&quot;], device=device) with torch.no_grad(): output = model(input_ids, attention_mask) batch[&quot;predicted_label&quot;] = torch.argmax(output.logits, axis=1) loss = torch.nn.functional.cross_entropy( output.logits, labels, reduction=&quot;none&quot; ) batch[&quot;loss&quot;] = loss # datasets requires list of NumPy array data types for k, v in batch.items(): batch[k] = v.cpu().numpy() return batch losses_ds = imdb_enc[&#39;test&#39;].map( loss_per_example, batched=True, batch_size=batch_size ) . It&#39;s then a simple matter to convert losses_ds to a pandas.DataFrame and sort by loss to find the examples where the model is most confused: . import pandas as pd pd.set_option(&quot;display.max_colwidth&quot;, None) losses_ds.set_format(&#39;pandas&#39;) losses_df = losses_ds[:][[&#39;label&#39;, &#39;predicted_label&#39;, &#39;loss&#39;]] # add the text column removed by the trainer losses_df[&#39;text&#39;] = imdb[&#39;test&#39;][&#39;text&#39;] losses_df.sort_values(&quot;loss&quot;, ascending=False).head() . label predicted_label loss text . 147 1 | 0 | 3.477502 | Was the script more fitting for a 30 minute sitcom? Yes, but they still make it work! I thought the actors did a fantastic job with an otherwise bland script, especially Jack Black and Christopher Walken. Most people on the board seem to really hate this film. I personally can&#39;t see how that could be, but Envy is just one of those film that you either love it or hate it. Much like Napoleon Dynamite and every Leslie Neilsen movie ever made. You either think it&#39;s one of the worst movies ever made or one of the funniest. Don&#39;t avoid this movie because of the reviews. Watch it and see if you&#39;re one of the ones who really like it! If you do, I guarantee it&#39;s worth your money. If you don&#39;t like it... well, now you know. | . 143 1 | 0 | 2.925410 | I would just like to say, that no matter how low budget the film is, it needs to be shown throughout this world the point to these movies. We don&#39;t read that much anymore, instead people want to see movies. Having this series out on DVD, has made me want to read the whole series, and want more. PLEASE MAKE ALL 8 MOVIES. Please don&#39;t change any of the characters either, it ruins the effect. Because I have grown to love the actors who have played the characters. PLEASE MAKE ALL 8 MOVIES. I want to see the message, and watch the message that these books and now movies are here to portray. We don&#39;t get that enough anymore. AWESOME JOB!!! | . 57 0 | 1 | 2.873445 | I like Brad Pitt enormously. He is an actor with brains and wit, not to mention face, pectorals and all the rest. Since I saw him in &quot;Thelma and Louise&quot; a thought has been bothering me, who does he remind me of? &quot;Troy&quot; did it for me. He is the new Brigitte Bardot. The differences are obvious of course. Male, American etc but Brigitte Bardot comes to mind nonetheless. He is so beautiful that he is at his most effective when he plays against it. &quot;Kalifornia&quot; &quot;12 Monkeys&quot; &quot;Fight Club&quot; &quot;Snatch&quot; His self deprecating humor makes him human, almost accessible. Fortunately &quot;Troy&quot; will soon be forgotten. Only still photographs with Pitt, semi naked in ravishing sprint positions will decorate the walls of legions of salivating fans. Strange, &quot;Das Boot&quot; is one of the great films of the second part of the 20th Century. What is Wolfgang Petersen doing directing this? Well, I suppose it would be very hard to say no at the chance of working with the new Brigitte Bardot. | . 151 1 | 0 | 2.861723 | SOLDIER is not as bad as many have made it out to be. I found the film to have some of the sacarstic, cynical humour like that in Paul Verhoven&#39;s Starship Troopers. The lack of dialogue and over the top action is deliberate and adds to the comic-book atmosphere.&lt;br /&gt;&lt;br /&gt;One particular trivia-bit stands out for me - Todd has the names of several space-war campaigns tattoo&#39;d onto his chest and one of these battles is TANNHAUSER GATE. For the oblivious ones out there, Tannhauser Gate is mentioned in Roy Batty&#39;s elegiac last lines in Blade Runner. To imagine that Todd could have fought alongside android troops like Roy is mind boggling to say the least. Maybe script writer David Peoples was nostalgic?&lt;br /&gt;&lt;br /&gt;I&#39;ll give this one 3 out of 5. | . 53 0 | 1 | 2.849806 | Reed Diamond plays a man suffering from amnesia who&#39;s been in a mental asylum for over a decade after he was found wondering the back roads with blood on his hands. The doctors want to test out an experimental new drug that&#39;ll return his lost memories if it works. But when the drugs give him hallucinations of a demon, he chooses to escape instead. While outside he befriends a young boy whose stepfather (Greg Grunberg) mistreats his mother, won&#39;t let her near the darkroom in his basement &amp; acts suspicious in general.&lt;br /&gt;&lt;br /&gt;While the general &#39;mystery&#39; of the film is a tad easy to identify way before it&#39;s revealed, I found Mr. Diamond&#39;s acting to be enthralling enough to keep my attention throughout. (In the interest of full disclosure, I&#39;ve been a huge fan of his since Homicide and his brief, but extremely pivotal, role in The Shield up through Journeyman &amp; Dollhouse) Not a great film nor a good one, but serviceable enough. Although I did like it better than the previous films that I&#39;ve seen from Director/writer Michael Hurst (Room 6, Pumkinhead 4, Mansquito)&lt;br /&gt;&lt;br /&gt;Eye Candy: one fleeting pair of boobs in a hallucination&lt;br /&gt;&lt;br /&gt;My Grade: C- | . 1. The non-padded version of this function comes from Leandro von Werra.↩ . .",
            "url": "https://lewtun.github.io/blog/til/nlp/huggingface/transformers/datasets/2021/01/01/til-data-collator.html",
            "relUrl": "/til/nlp/huggingface/transformers/datasets/2021/01/01/til-data-collator.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Highlights from ICML 2020",
            "content": "This year I had the opportunity to attend the International Conference on Machine Learning (ICML) and decided to highlight some of the talks I found especially interesting. Although the conference was hosted entirely online, this provided two key benefits over attending in person: . Clash resolution: with 1,088 papers accepted, it is inevitable that multiple talks of interest would clash in the timetable. Watching the pre-recorded presentations in my own time provided a simple solution, not to mention the ability to quickly switch to a new talk if desired. | Better Q&amp;A sessions: at large conferences it is not easy to get your questions answered directly after a talk, usually because the whole session is running overtime and the moderator wants to move onto the next speaker. By having two (!) dedicated Q&amp;A sessions for each talk, I found the discussions to be extremely insightful and much more personalised. | . Since I&#39;m resigned to being in quarantine until 2050, I hope other virtual conferences will adopt a similar format. Conference highlights are below! . Transformers . Generative Pretraining from Pixels . . Predicting the next pixel with a GPT-2 scale model yields high quality representations. The best representations lie in the middle of the network. . This talk showed that with enough compute, it is possible to adapt transformer architectures to images and achieve strong results in self-supervised learning benchmarks. Dubbed iGPT, this approach relies on a three-step process: . Downsize the images, cluster the RGB pixel values to create a 9-bit colour map, and reshape to 1D.1 | Pre-train on either an autoregressive next pixel or masked pixel prediction task. | Evaluate the quality of the learned representations on downstream tasks. | One surprising result of the linear probe2 experiments is that representation quality tends to be highest in the middle of the network. . I think this work provides a compelling example of Sutton&#39;s &quot;bitter lesson&quot; . Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features. But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better. . but takes it one step further by discarding knowledge of the 2D structure in images entirely! . Although the iGPT models are 2-30 times larger than ResNet-152, I expect it is only a matter of time before people find ways to make this approach more efficient. In the meantime, it&#39;s nice to see that the pre-trained models have been open-sourced and a port to HuggingFace&#39;s transformers library is already underway. . Retrieval Augmented Language Model Pre-Training . . Augmenting language models with knowledge retrieval sets a new benchmark for open-domain question answering. . I liked this talk a lot because it takes a non-trivial step towards integrating world knowledge into language models and addresses Gary Marcus&#39; common complaint that data and compute aren&#39;t enough to produce Real Intelligence&trade;. . To integrate knowledge into language model pretraining, this talk proposes adding a text retriever that is learned during the training process. Unsurprisingly, this introduces a major computational challenge because the conditional probability now involves a sum over all documents in a corpus $ mathcal{Z}$: . $$ p(y|x) = sum_{z in mathcal{Z}} p(y|x,z)p(z) ,.$$ . To deal with this, the authors compute an embedding for every document in the corpus and then use Maximum Inner Product Search algorithms to find the approximate top $k$ documents. The result is a hybrid model that significantly outperforms other approaches in open-domain question answering. . Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention . . A clever choice of kernel reduces the computational complexity of attention from $O(N^2)$ to $O(N)$. Generate images 4000x faster than vanilla transformers :fire:. . It&#39;s refreshing to see a transformer talk that isn&#39;t about using a &quot;bonfire worth of GPU-TPU-neuromorphic wafer scale silicon&quot;4 to break NLP benchmarks. This talk observes that the main bottleneck in vanilla transformer models is the softmax attention computation . $$ V&#39; = mathrm{softmax} left( frac{QK^T}{ sqrt{D}} right) V $$ . whose time and space complexity is $O(N^2)$ for sequence length $N$. To get around this, the authors first use a similarity function to obtain a generalised form of self-attention . $$ V_i&#39; = frac{ sum_j mathrm{sim}(Q_i, K_j)V_j}{ sum_j mathrm{sim}(Q_i, K_j)} $$ . which can be simplified via a choice of kernel and matrix associativity: . $$V_i&#39; = frac{ phi(Q_i)^T sum_j phi(K_j)V_j^T}{ phi(Q_i)^T sum_j phi(K_j)} ,. $$ . The result is a self-attention step that is $O(N)$ because the sums in the above expression can be computed once and reused for every query. In practice, this turns out to be especially powerful for inference, with speed-ups of 4000x reported in the talk! . The authors go on to show that their formulation can also be used to express transformers as RNNs, which might be an interesting way to explore the shortcomings of these large langauge models. . XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation . . A new benchmark to test zero-shot cross-lingual transfer from English to 39 diverse languages. . In this talk, the authors introduce the XTREME benchmark to evaluate the ability of multilingual representations to generalise across 40 languages and 9 tasks. To evaluate a model in XTREME, the main idea is to follow a three-stage recipe: . Pre-train on a large corpus of multilingual text. | Fine-tune on English data for each task. | Evaluate the model on zero-shot transfer performance, e.g. evaluate the accuracy on a German text classification task. | English is chosen for fine-tuning because it&#39;s the langauge with the most labelled data, and the authors employ a neat trick using Google Translate to generate proxy test sets for the tasks where a pre-existing translation does not exist. . Although not strictly about Transformers, the baseline models for this benchmark are all variants of the Transformer architecture, and the authors find that XLM-R achieves the best zero-shot transfer performance across all languages in each task. What I especially like about XTREME is that the tasks are designed to be trainable on a single GPU for less than a day. This should make it possible for research labs with tight budgets to create competitive models, where the gains in performance are likely to come from architectural design rather than simply scaling-up the compute. . I&#39;m excited about this benchmark because I expect it will produce models that have a direct impact on my professional work in Switzerland. With four national languages and a smattering of English, building natural language applications that serve the whole population is a constant challenge. . Time series . Set Functions for Time Series . . High-performance classification for multivariate, irregularly sampled time series. . Time series seems to be the neglected child of machine learning research, so I was excited to see a talk that combines a lot of cool ideas like Deep Sets, attention, and positional encodings in a new architecture. The motivation for this work is based on the observation that: . Imputation techniques for sparse or irregularly sampled time series introduce bias or don&#39;t make sense at all.5 | Many time series of practical interest are multivariate in nature, and often with unaligned measurements | . The authors note that for time series classification tasks, the order of input measurements is not important and thus one can reframe the problem as classifing a set of observations. By representing each observation as a tuple $(t_i, z_i, m_i)$ of timestamp $t_i$, observation $z_i$ and indicator $m_i$, an entire time series can be written as . $$ mathcal{S} = {(t_1,z_1,m_1), ldots , (t_M, z_M, m_M) }$$ . The goal is then to learn a function $f: mathcal{S} to mathbb{R}^C$ which the authors do via the Deep Sets approach to obtain a highly-scalable architecture. One aspect I especially liked in this talk is the use of attention to visualise which observations contributed to the model output. . . In industry it is quite common for domain experts to have a different mental model on how to interpret the predictions from your model, and visualisations like these could be really handy as a common discussion point. I&#39;m quite excited to see if I can use this approach to tackle some thorny time series problems at work! . Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure . . A new unsupervised anomaly detection algorithm for IoT devices. . This talk proposes a new technique to distinguish &quot;normal&quot; from &quot;abnormal&quot; events in streams of telemetry data from IoT devices. Like almost every real-world anomaly detection problem, one rarely has training data with labelled anomalies.6 . The main novelty in this talk is a method to deal with the lack of labels by framing the problem as a binary classification task, where one class contains positive (mostly &quot;normal&quot;) samples while the other contains negative samples that are supposed to represent the space of anomalies. A sample ratio parameter $r_s$ controls the ratio of negative to positive sample sizes and acts as a sort of hyperparameter or threshold that is tuned. . Although this method will generate false positive and false negative labelling errors, the author notes that the former are rare (by definition) and the latter decay exponentially for high-dimensional time series. Once the &quot;labelled&quot; dataset is created, it is then a simple matter to train a classifier and the talk notes that both neural nets and random forests perform comparably well. . One really neat aspect of this work is that it also introduces a novel way to interpret anomalies for root-cause analysis. The aim here is to figure out which dimensions contribute most to an anomaly score and the talk proposes a method based on integrated gradients. Here the basic idea is to identify which dimensions of the time series must be changed to transform an anomalous point into a normal one. . I think the methods in this paper can have a direct impact in my day job and I&#39;m interested to see how it performs on the challenging Numenta Anomaly Benchmark. Since the code is open-sourced, this will be a nice weekend project! . Physics . Learning to Simulate Complex Physics with Graph Networks . . A single architecture creates high-fidelity particle simulations of various interacting materials. . I&#39;m a sucker for flashy demos and this talk from DeepMind didn&#39;t disappoint. They propose an &quot;encode-process-decode&quot; architecture to calculate the dynamics of physical systems, where particle states are represented as graphs and a graph neural network learns the particle interactions. . . During training, the model predicts each particle&#39;s position and velocity one timestep into the future, and these predictions are compared against the ground-truth values of a simulator. Remarkably, this approach generalises to thousands of timesteps at test time, even under different initial conditions and an order of magnitude more particles!3 . I think this work is a great example of how machine learning can help physicists build better simulations of complex phenomena. It will be interesting to see whether this approach can scale to systems with billions of particles, like those found in dark matter simulations or high-energy collisions at the Large Hadron Collider. . 1. Downscaling is needed because naively training on a $224^2 times 3$ sequence length would blow up the memory of the largest TPU!↩ . 2. A linear probe refers to using the model as a feature extractor and passing those features through a linear model like logistic regression.↩ . 3. The authors ascribe this generalisation power to the fact that each particle is only aware of local interactions in some &#39;connectivity radius&#39;, so the model is flexible enough to generalise to out-of-distribution inputs.↩ . 4. Quote from Stephen Merity&#39;s brilliant Single Headed Attention RNN: Stop Thinking With Your Head.↩ . 5. For example, in a medical context where a patient&#39;s vitals may only be measured if the doctor orders a test.↩ . 6. And even if you did, supervised approaches tend to experience &#39;model rot&#39; quite quickly when dealing with vast streams of data.↩ .",
            "url": "https://lewtun.github.io/blog/research/conference/2020/07/31/icml2020.html",
            "relUrl": "/research/conference/2020/07/31/icml2020.html",
            "date": " • Jul 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "G’day! I’m a data scientist at Swisscom, a major telecom company in Switzerland. I have several years of experience building machine learning-powered applications for startups and enterprises, mostly in the domains of time series and natural language processing. . In a previous life I was a theoretical physicist, where I developed quantum field theories to make sense of data from collider and dark matter experiments. . Outside of work, I love playing guitar, trail running, and contributing to open-source projects. . I use this blog to share machine learning ideas and techniques that I find interesting or (more likely) to remind my future self of things I’ve forgotten :see_no_evil:. .",
          "url": "https://lewtun.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lewtun.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}