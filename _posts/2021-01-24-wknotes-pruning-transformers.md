---
keywords: fastai
title: "Weeknotes: Fine-pruning transformers, universal data augmentation"
comments: false
categories: [weeknotes,nlp,huggingface,transformers,compression,few-shot]
badges: false
image: images/prune-tuning.png
nb_path: _notebooks/2021-01-24-wknotes-pruning-transformers.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-24-wknotes-pruning-transformers.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This week I split my time between wrapping up a book chapter on abstractive summarisation, trying to get UDA to work, and getting my hands dirty with movement pruning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A-first-look-at-movement-pruning">A first look at movement pruning<a class="anchor-link" href="#A-first-look-at-movement-pruning"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This week I focused on a compression technique for Transformers called <em>pruning</em>, whose goal is to selectively delete the weights of a model according to some importance criterion. In particular I wanted to understand how movement pruning{% fn 1 %} worked and how I could adapt <a href="https://twitter.com/SanhEstPasMoi?s=20">Victor Sanh's</a> implementation to run in Jupyter notebooks with the <code>Trainer</code> API from <code>transformers</code>.</p>
<p>The basic idea behind movement pruning is to <em>gradually</em> remove weights during <em>fine-tuning</em> such that the model becomes progressively <em>sparser</em>. As the authors observe, this "fine-pruning" approach addresses one of the main problems with other approaches like <em>magnitude pruning</em> that are designed for pure supervised learning tasks:</p>
<blockquote><p>While magnitude pruning is highly effective for standard supervised learning, it is inherently less useful in the transfer learning regime. In supervised learning, weight values are primarily determined by the end-task training data. In transfer learning, weight values are mostly predetermined by the original model and are only fine-tuned on the end task. This prevents these methods from learning to prune based on the fine-tuning step, or “fine-pruning.”</p>
</blockquote>
<p>Mathematically, the way most pruning methods work is to calculate a matrix ${\bf S}$ of <em>importance scores</em> and then select the top-$v$ percent of weights by importance:$$ \mathrm{Top}_v({\bf S})_{ij} = \left\{ \begin{aligned} 1 &amp;&amp; \mathrm{if} \, S_{ij} \mathrm{ \,in\, top\, } v\% \\ 0 &amp;&amp; \mathrm{otherwise}\end{aligned}  \right.$$
From these scores we can then define a <em>mask</em> ${\bf M} \in \{0,1\}^{n\times n}$ that masks the weights during the forward pass with some input $x_i$ and effectively creates a sparse network:</p>
<p>{% raw %}
$$ a_i = W_{ik}M_{ik}x_k \,.$$
{% endraw %}</p>
<p>For example, magnitude pruning calculates the scores according to the magnitude of the weights ${\bf S} = \left(\mid W_{ij} \mid\right)_{1\leq j, j\leq n}$ and then the masks are derived from ${\bf M} = \mathrm{Top}_v({\bf S})$.</p>
<p>The key novelty with movement pruning is that both the weights <em>and</em> the scores are <em>learned</em> during fine-tuning. This implies that in the backward pass, we also track the gradient of the loss ${\cal L}$ with respect to $S_{ij}$:{% fn 2 %}</p>
<p>{% raw %}
$$ \frac{\partial{\cal L}}{\partial S_{ij}} = \frac{\partial {\cal L}}{\partial a_i}\frac{\partial a_i}{\partial S_{ij}} = \frac{\partial {\cal L}}{\partial a_i}W_{ij}x_j$$
{% endraw %}</p>
<p>Once the scores are learned, it is then straightforward to generate the mask using ${\bf M} = \mathrm{Top}_v({\bf S})$. The authors also propose a "soft" version of movement pruning where instead of picking the top-$v$% of weights, one uses a global threshold $\tau$ to define the binary mask: ${\bf M} = ({\bf S} &gt; \tau)$.</p>
<p>The paper has a nice visualisation of how the pretrained weights of BERT are pruned during fine-tuning and shows how magnitude pruning tends to make the pruning decision mostly on the basis of the pretrained weights (i.e. weights that have small absolute value during pre-training get pruned).</p>
<p><img src="/blog/images/copied_from_nb/my_icons/mag-vs-mov.png" alt=""></p>
<p>In their experiments, the authors use a cubic sparsity scheduler to increase the amount of sparsity after some $t_i$ steps of warmp-up:</p>
<p>{% raw %}
$$v^{(t)} = v_f + (v_i-v_f)\left(1 - \frac{t-t_i}{N\Delta t}\right)^3 \,.$$
{% endraw %}</p>
<p>The results for both hard and soft movement pruning on SQuAD and other benchmarks are quite impressive, especially in the high-sparsity regimes where less than 5% of the weights are retained!</p>
<p><img src="/blog/images/copied_from_nb/my_icons/mov-pruning-results.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implementing-movement-pruning">Implementing movement pruning<a class="anchor-link" href="#Implementing-movement-pruning"> </a></h2><p>As noted above, I wanted to adapt Victor Sanh's implementation to work with the <code>Trainer</code> API from <code>transformers</code> so that I can run it in a Jupyter notebook. Implementing the <code>Trainer</code> itself was pretty straightforward and I was able to reuse a lot of Victor's code with minor adjustments. The first thing to do was override the <code>compute_loss</code> function as follows:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span> 
    <span class="n">threshold</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_schedule_threshold</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;threshold&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">threshold</span>     
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">outputs</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
<p>Here we use the sparsity scheduler to get the threshold value $\tau$ needed for soft movement pruning, add it to the inputs and then extract the loss from the forward pass. The next step was to override the <code>create_optimizer_and_scheduler</code> function to account for the fact that there is a learning rate $\alpha_S$ associated with calculating the scores matrix:</p>
<p>{% raw %}
$$ S_{ij}^{(T)} = -\alpha_S \sum_{t&lt;T} \left( \frac{\partial {\cal L}}{\partial W_{ij}}\right)^{(t)} W_{ij}^{(t)} $$
{% endraw %}</p>
<p>In practice, this amounts to adding a term to the parameters we wish to optimize over</p>
<div class="highlight"><pre><span></span><span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> 
                   <span class="k">if</span> <span class="s2">&quot;mask_score&quot;</span> <span class="ow">in</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">],</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">mask_scores_learning_rate</span><span class="p">,</span>
    <span class="p">},</span> <span class="o">...</span>
<span class="p">]</span>
</pre></div>
<p>so that the final function takes the form:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_optimizer_and_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">):</span>
    <span class="n">no_decay</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="s2">&quot;LayerNorm.weight&quot;</span><span class="p">]</span>
    <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> 
                       <span class="k">if</span> <span class="s2">&quot;mask_score&quot;</span> <span class="ow">in</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">],</span>
            <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">mask_scores_learning_rate</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">p</span>
                <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
                <span class="k">if</span> <span class="s2">&quot;mask_score&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> 
                <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">nd</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">nd</span> <span class="ow">in</span> <span class="n">no_decay</span><span class="p">)</span>
            <span class="p">],</span>
            <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">p</span>
                <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
                <span class="k">if</span> <span class="s2">&quot;mask_score&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> 
                <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">nd</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">nd</span> <span class="ow">in</span> <span class="n">no_decay</span><span class="p">)</span>
            <span class="p">],</span>
            <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">optimizer_grouped_parameters</span><span class="p">,</span> 
                           <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">adam_epsilon</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">,</span> 
        <span class="n">num_training_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">t_total</span><span class="p">)</span>
</pre></div>
<p>So far, so good ... but what I had not appreciated is that one needs <em>special</em> model classes to deal with sparse matrices! In Victor's implementation, this requires a wholescale rewrite of the BERT classes to replace all the <code>torch.nn.Linear</code> layers with a custom <code>MaskedLinear</code> layer and additional parameters to calculate the adaptive mask in the forward pass.</p>
<p>Although there is <a href="https://discuss.huggingface.co/t/hugging-face-reads-01-2021-sparsity-and-pruning/3144/4?u=lewtun">no plan</a> to include these masked versions of BERT into the main <code>transformers</code> library, <a href="https://twitter.com/madlag?s=20">François Lagunas</a> at HuggingFace pointed me to work he's done on making <a href="https://github.com/huggingface/pytorch_block_sparse">sparse matrices efficient in PyTorch</a>.</p>
<p>In any case, I went ahead with Victor's masked models and ran a first set of experiments using 10% of the SQuAD data. To warmup, I used Victor's scripts as a benchmark and observed some peculiar features of fine-pruning: the metrics are flat for half the training before suddenly shooting up! Similarly, the loss gets <em>worse</em> before getting better. This is somewhat surprising, since fine-tuning usually gets most of the performance in the first 1-2 epochs of training before plateauing.</p>
<p><img src="/blog/images/copied_from_nb/my_icons/pruning-scores.png" alt=""></p>
<p>So far I have not been able to reproduce these results in my implementation, with my model failing to recover from the charactersitic dip in performance during training:</p>
<p><img src="/blog/images/copied_from_nb/my_icons/pruning-scores-fail.png" alt=""></p>
<p>So my focus for next week is to figure out what's going wrong and gradually scale-out to fine-pruning on the full SQuAD dataset!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Universal-data-augmentation">Universal data augmentation<a class="anchor-link" href="#Universal-data-augmentation"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the last two weeks, <a href="https://twitter.com/lvwerra?s=20">Leandro von Werra</a> and I have been dabbling in few-shot learning with Google's <a href="https://arxiv.org/abs/1904.12848"><em>Unsupervised Data Augmentation for Consistency Training</em></a> or UDA for short. For the book we're working on, one idea is to provide readers with a solution to a common problem in industry: what do you do when you've got tons of unlabelled text data and only 10s-100s of labelled examples?</p>
<p>The UDA paper proposes an elegant approach to this problem by applying data augmentation on the unlabelled data and then minimising the KL divergence between the model's predicted probability distribution for the raw and augmentated data. This "unsupervised consistency loss" is then added to the standard cross-entropy loss coming from the labelled examples and the model is trained jointly across the two tasks.</p>
<p><img src="/blog/images/copied_from_nb/my_icons/uda.png" alt=""></p>
<p>The paper reports some spectacular results: using just <em>20 examples</em> from IMDB, UDA gets an error-rate that surpasses BERT-large fine-tuned on the full 25k examples in the training set!</p>
<p>There was just one hitch: the Google implementation is in <a href="https://github.com/google-research/uda/issues/8">Python2</a> and Tensorflow v1 🤮</p>
<p><img src="/blog/images/copied_from_nb/my_icons/uda-python.png" alt=""></p>
<p>Being allergic to both, we decided to see if we could reproduce the results from an open-source port to PyTorch. In hindsight, this turned out to be a foolish decision because now we were debugging against 3 frameworks! It was also a humbling lesson in not believing what is reported in some random repo you find on the internet 😉.</p>
<p>So in the end, I bit the bullet and decided to run Google's implementation which unsurprisingly worked out of the box.{% fn 3 %} With just 10k steps and a few hours of training on a single GPU, UDA can indeed achieve &gt; 90% accuracy on IMBD:</p>

<pre><code>=== step 500 ===
INFO:tensorflow:  eval_classify_loss = 0.3957828
INFO:tensorflow:  eval_classify_accuracy = 0.57844
INFO:tensorflow:  loss = 0.80444646
=== step 1000 ===
INFO:tensorflow:  eval_classify_loss = 0.68793213
INFO:tensorflow:  eval_classify_accuracy = 0.56504
INFO:tensorflow:  loss = 1.4864826
=== step 2000 ===
INFO:tensorflow:  eval_classify_loss = 0.14758773
INFO:tensorflow:  eval_classify_accuracy = 0.89524
INFO:tensorflow:  loss = 0.71094906
=== step 10000 ===
INFO:tensorflow:  eval_classify_loss = 0.23858581
INFO:tensorflow:  eval_classify_accuracy = 0.91296
INFO:tensorflow:  loss = 0.23858581</code></pre>
<p>So now that we're confident UDA really works, the next step will be to do a proper port to PyTorch - yay!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TIL-this-week">TIL this week<a class="anchor-link" href="#TIL-this-week"> </a></h2><ul>
<li><a href="https://lewtun.github.io/blog/til/nlp/pytorch/2021/01/24/til-slicing-torch-datasets.html">Slicing PyTorch Datasets</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{{ '<a href="https://arxiv.org/abs/2005.07683"><em>Movement Pruning: Adaptive Sparsity by Fine-Tuning</em></a> by Victor Sanh, Thomas Wolf, Alexander M. Rush (2020)' | fndetail: 1 }}</p>
<p>{{ 'In a new term for me, this estimator is called straight-through because the top-$v$ function is ignored in the backward pass.' | fndetail: 2 }}</p>
<p>{{ "Well, <em>almost</em>. It took me a while to realise that when TensorFlow's <code>TPUEstimator</code> says it's running on a CPU, it's actually running on a GPU 🤷." | fndetail: 3 }}</p>

</div>
</div>
</div>
</div>
 

