---
keywords: fastai
title: "Weeknotes: Question answering with ðŸ¤— transformers, mock interviews"
comments: false
categories: [weeknotes,nlp,huggingface,transformers]
badges: false
nb_path: _notebooks/2021-01-10-wknotes-question-answering.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-10-wknotes-question-answering.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>During my PhD and postdoc, I kept detailed research notes that I would often revisit to reproduce a lengthy calculation or simply take stock of the progress I'd made on my projects. For various reasons, I dropped this habit when I switched to industry and nowadays find myself digging out code snippets or techniques from a tangle of Google Docs,  Git repositories, and Markdown files that I've built up over the years.</p>
<p>To break this anti-pattern, I've decided to "work in public" as much as possible this year, mostly in the form of <a href="https://www.urbandictionary.com/define.php?term=TIL">TILs</a> and weeknotes. Here, I am drawing inspiration from the prolific <a href="https://twitter.com/simonw?s=20">Simon Willison</a>, whose <a href="https://simonwillison.net/">blog</a> meticulously documents the development of his open-source projects.{% fn 1 %}</p>
<p>To that end, here's the first weeknotes of the year - hopefully they're not the last!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Question-answering">Question answering<a class="anchor-link" href="#Question-answering"> </a></h2><p>This week I've been doing a deep dive into extractive question answering as part of a book chapter I'm writing on compression methods for Transformers. Although I'd built a question answering PoC with BERT in the dark ages of 2019, I was curious to see how the implementation could be done in the <code>transformers</code> library, specifically with a custom <code>Trainer</code> class and running everything inside Jupyter notebooks.</p>
<p>Fortunately, <a href="https://twitter.com/GuggerSylvain?s=20">Sylvain Gugger</a> at HuggingFace had already implemented</p>
<ul>
<li>A <a href="https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb">tutorial</a> on fine-tuning language models for question answering, but without a custom <code>Trainer</code></li>
<li>A custom <code>QuestionAnsweringTrainer</code> as part of the <a href="https://github.com/huggingface/transformers/tree/master/examples/question-answering">question answering scripts</a> in <code>transformers</code>.</li>
</ul>
<p>so my warm-up task this week was to simply merge the two in a single notebook and fine-tune <code>bert-base-uncased</code> on SQuAD v1. I implemented very scrappy version that achieves this in my <code>transformerlab</code> repository, and the main lesson I learnt is that</p>
<blockquote><p>Dealing with sequence length is tricky for long documents</p>
</blockquote>
<p>Transformer models can only process a finite number of input tokens, a property usually referred to as the maximum sequence length. As described in Sylvain's tutorial, naive truncation of documents for question answering is problematic because</p>
<blockquote><p>removing part of the the context might result in losing the answer we are looking for.</p>
</blockquote>
<p>The solution is to apply a <em>sliding window</em>{% fn 2 %} to the input context, so that long contexts end up producing <em>multiple</em> features. An example from the tutorial shows how this works by introducing two new hyperparameters <code>max_length</code> and <code>doc_stride</code> that control the degree of overlap (bold shows the overlapping region):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 â€“ 2010 season. the team is coached by mike brey, who, as of the 2014 â€“ 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the <em><strong>championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were</strong></em> [SEP]</p>
<p>[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Remarkably, <code>transformers</code> supports this preprocessing logic out of the box, so one just has to specify a few arguments in the tokenizer:</p>
<div class="highlight"><pre><span></span><span class="n">tokenized_example</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">example</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">],</span>
    <span class="n">example</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="s2">&quot;only_second&quot;</span><span class="p">,</span>
    <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="n">doc_stride</span><span class="p">)</span>
</pre></div>
<p>One drawback from this approach is that it introduces significant complexity into the data preparation step; with multiple features per example, one needs to do some heavy wrangling to pick out the start and end positions of each answer.</p>
<p>Since my book chapter is primarily about compression methods, I may opt for the simpler, but less rigourous approach of  truncating the long examples (as done in the <code>transformer</code> <a href="https://huggingface.co/transformers/custom_datasets.html#question-answering-with-squad-2-0">docs</a>). An alternative would be to adopt the "retriever-reader" architecture that I used in my PoC (where I split long documents into smaller paragraphs), but that introduces new concepts and its own complexity that will likely be distracting for the reader.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Running-a-mock-interview">Running a mock interview<a class="anchor-link" href="#Running-a-mock-interview"> </a></h2><p>A friend of mine is applying for a research scientist position and we thought it would be fun to run a couple of mock interviews together. Since the position is likely to involve Transformers, I asked my friend a few GPT-related questions (e.g. how does the architecture differ from BERT and what is the difference between GPT / GPT-2 and GPT-3?), followed by a coding session to see how fast one could implement GPT from scratch. The goal was to approach a skeleton of <a href="https://karpathy.ai/">Andrej Karpathy's</a> excellent <a href="https://github.com/karpathy/minGPT">minGPT</a> implementation</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include twitter.html content='<a href="https://twitter.com/karpathy/status/1295410274095095810?s=20">https://twitter.com/karpathy/status/1295410274095095810?s=20</a>' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>and the experience taught me a few lessons:</p>
<ul>
<li>There's a significant difference between being a power-user of a library like <code>transformers</code> versus deeply knowing how every layer, activation function, etc in a deep neural architecture is put together. Running the interview reminded me that I should aim to block time per week to hone the foundations of my machine learning knowledge.</li>
<li>Open-ended coding interviews like this are way more fun to conduct than the usual LeetCode / HackerRank problems one usually encounters in tech. To me, they resemble a pair-programming interaction that gives the interviewer a pretty good feel for what it would be like to work closely with the candidate. Something to remember the next time I'm interviewing people for a real job!</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Papers-this-week">Papers this week<a class="anchor-link" href="#Papers-this-week"> </a></h2><p>This week I've been mostly reading papers on distilling Transformers and how to improve few-shot learning <em>without</em> resorting to massive scaling:</p>
<ul>
<li><a href="https://arxiv.org/abs/1910.01108"><em>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</em></a> by Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf (2019)</li>
<li><a href="https://arxiv.org/abs/2010.13382"><em>FastFormers: Highly Efficient Transformer Models for Natural Language Understanding</em></a> by Young Jin Kim and Hany Hassan Awadalla (2020)</li>
<li><a href="https://arxiv.org/abs/2006.15315"><em>Uncertainty-aware Self-training for Text Classification with Few Labels</em></a> by Subhabrata Mukherjee and Ahmed Hassan Awadallah (2020)</li>
</ul>
<p>This week also coincided with the release of OpenAI's <a href="https://openai.com/blog/dall-e/">DALL-E</a> which, although light on implementation details, provided a fun interface to see how far you can push the limits of text-to-image generation:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/my_icons/dalle.png" alt="" title="The DALL-E blog post has many examples involving Capybaras, which happen to be my favourite animal."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TIL-this-week">TIL this week<a class="anchor-link" href="#TIL-this-week"> </a></h2><ul>
<li><a href="https://lewtun.github.io/blog/til/2021/01/07/til-poll-api-with-bash.html">Polling a web service with bash and jq</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{{ 'Even down to the level of reviewing his own <a href="https://github.com/simonw/datasette/pull/1117">pull requests</a>!' | fndetail: 1 }}</p>
<p>{{ 'We want a <em>sliding</em> window instead of a <em>tumbling</em> one because the answer might appear across the boundary of the two windows.' | fndetail: 2 }}</p>

</div>
</div>
</div>
</div>
 

