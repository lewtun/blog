<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Highlights from ICML 2020 | Lewis Tunstall</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Highlights from ICML 2020" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Transformers :robot:, time series :chart_with_upwards_trend:, and a little bit of physics :apple:." />
<meta property="og:description" content="Transformers :robot:, time series :chart_with_upwards_trend:, and a little bit of physics :apple:." />
<link rel="canonical" href="https://lewtun.github.io/blog/research/conference/2020/07/14/icml2020.html" />
<meta property="og:url" content="https://lewtun.github.io/blog/research/conference/2020/07/14/icml2020.html" />
<meta property="og:site_name" content="Lewis Tunstall" />
<meta property="og:image" content="https://lewtun.github.io/blog/images/icml.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-14T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-07-14T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://lewtun.github.io/blog/research/conference/2020/07/14/icml2020.html"},"description":"Transformers :robot:, time series :chart_with_upwards_trend:, and a little bit of physics :apple:.","image":"https://lewtun.github.io/blog/images/icml.png","@type":"BlogPosting","url":"https://lewtun.github.io/blog/research/conference/2020/07/14/icml2020.html","headline":"Highlights from ICML 2020","dateModified":"2020-07-14T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://lewtun.github.io/blog/feed.xml" title="Lewis Tunstall" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Highlights from ICML 2020 | Lewis Tunstall</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Highlights from ICML 2020" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Transformers :robot:, time series :chart_with_upwards_trend:, and a little bit of physics :apple:." />
<meta property="og:description" content="Transformers :robot:, time series :chart_with_upwards_trend:, and a little bit of physics :apple:." />
<link rel="canonical" href="https://lewtun.github.io/blog/research/conference/2020/07/14/icml2020.html" />
<meta property="og:url" content="https://lewtun.github.io/blog/research/conference/2020/07/14/icml2020.html" />
<meta property="og:site_name" content="Lewis Tunstall" />
<meta property="og:image" content="https://lewtun.github.io/blog/images/icml.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-14T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-07-14T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://lewtun.github.io/blog/research/conference/2020/07/14/icml2020.html"},"description":"Transformers :robot:, time series :chart_with_upwards_trend:, and a little bit of physics :apple:.","image":"https://lewtun.github.io/blog/images/icml.png","@type":"BlogPosting","url":"https://lewtun.github.io/blog/research/conference/2020/07/14/icml2020.html","headline":"Highlights from ICML 2020","dateModified":"2020-07-14T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://lewtun.github.io/blog/feed.xml" title="Lewis Tunstall" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper">
<a class="site-title" rel="author" href="/blog/">Lewis Tunstall</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Highlights from ICML 2020</h1>
<p class="page-description">Transformers <img class="emoji" title=":robot:" alt=":robot:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png" height="20" width="20">, time series <img class="emoji" title=":chart_with_upwards_trend:" alt=":chart_with_upwards_trend:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png" height="20" width="20">, and a little bit of physics <img class="emoji" title=":apple:" alt=":apple:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f34e.png" height="20" width="20">.</p>
<p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-07-14T00:00:00-05:00" itemprop="datePublished">
        Jul 14, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i> 
      
        <a class="category-tags-link" href="/blog/categories/#research">research</a>
         
      
        <a class="category-tags-link" href="/blog/categories/#conference">conference</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2">
<a href="#Transformers">Transformers </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Generative-Pretraining-from-Pixels">Generative Pretraining from Pixels </a></li>
<li class="toc-entry toc-h3"><a href="#Retrieval-Augmented-Language-Model-Pre-Training">Retrieval Augmented Language Model Pre-Training </a></li>
<li class="toc-entry toc-h3"><a href="#Transformers-are-RNNs:-Fast-Autoregressive-Transformers-with-Linear-Attention">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention </a></li>
<li class="toc-entry toc-h3"><a href="#XTREME:-A-Massively-Multilingual-Multi-task-Benchmark-for-Evaluating-Cross-lingual-Generalisation">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation </a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#Time-series">Time series </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Set-Functions-for-Time-Series">Set Functions for Time Series </a></li>
<li class="toc-entry toc-h3"><a href="#Interpretable,-Multidimensional,-Multimodal-Anomaly-Detection-with-Negative-Sampling-for-Detection-of-Device-Failure">Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure </a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#Physics">Physics </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Learning-to-Simulate-Complex-Physics-with-Graph-Networks">Learning to Simulate Complex Physics with Graph Networks </a></li>
</ul>
</li>
</ul>
<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-14-icml2020.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This year I had the opportunity to attend the <a href="https://icml.cc/">International Conference on Machine Learning</a> (ICML) and decided to highlight some of the talks I found especially interesting. Although the conference was hosted entirely online, this provided two key benefits over attending in person:</p>
<ul>
<li>
<strong>Clash resolution:</strong> with <a href="https://syncedreview.com/2020/06/01/icml-2020-announces-accepted-papers/#:~:text=Conference%20Industry-,ICML%202020%20Announces%20Accepted%20Papers,the%20prestigious%20machine%20learning%20conference.">1,088 papers accepted</a>, it is inevitable that multiple talks of interest would clash in the timetable. Watching the pre-recorded presentations in my own time provided a simple solution, not to mention the ability to quickly switch to a new talk if desired.</li>
<li>
<strong>Better Q&amp;A sessions:</strong> at large conferences it is not easy to get your questions answered directly after a talk, usually because the whole session is running overtime and the moderator wants to move onto the next speaker. By having two (!) dedicated Q&amp;A sessions for each talk, I found the discussions to be extremely insightful and much more personalised.</li>
</ul>
<p>Since I'm resigned to being in quarantine until 2050, I hope other conferences will adopt a similar format. Highlights below!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformers">
<a class="anchor" href="#Transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformers<a class="anchor-link" href="#Transformers"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Generative-Pretraining-from-Pixels">
<a class="anchor" href="#Generative-Pretraining-from-Pixels" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6022-Paper.pdf">Generative Pretraining from Pixels</a><a class="anchor-link" href="#Generative-Pretraining-from-Pixels"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/my_icons/igpt.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Predicting the next pixel with a GPT-2 scale model yields high quality representations. The best representations lie in the middle of the network.</em></p>
<p>This talk shows that with enough compute, it is possible to adapt transformer architectures to images and achieve strong results in self-supervised learning benchmarks. Dubbed iGPT, this approach relies on a three-step process:</p>
<ol>
<li>Downsize the images, cluster the RGB pixel values to create a 9-bit colour map, and reshape to 1D.<sup id="fnref-1" class="footnote-ref"><a href="#fn-1">1</a></sup>
</li>
<li>Pre-train on either an autoregressive next pixel or masked pixel prediction task.</li>
<li>Evaluate the quality of the learned representations on downstream tasks.</li>
</ol>
<p>One surprising result of the linear probe<sup id="fnref-2" class="footnote-ref"><a href="#fn-2">2</a></sup> experiments is that representation quality tends to be highest in the <em>middle</em> of the network.</p>
<p>I think this work provides a compelling example of Sutton's <a href="http://incompleteideas.net/IncIdeas/BitterLesson.html">"bitter lesson"</a></p>
<blockquote>
<p>Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features. But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better.</p>
</blockquote>
<p>but takes it one step further by discarding knowledge of the 2D structure in images entirely!</p>
<p>Although the iGPT models are 2-30 times larger than ResNet-152, I expect it is only a matter of time before people find ways to make this approach more efficient. In the meantime, it's nice to see that the pre-trained models have been <a href="https://github.com/openai/image-gpt">open-sourced</a> and a <a href="https://github.com/huggingface/transformers/issues/5088">port</a> to HuggingFace's transformers library is already underway.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Retrieval-Augmented-Language-Model-Pre-Training">
<a class="anchor" href="#Retrieval-Augmented-Language-Model-Pre-Training" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3102-Paper.pdf">Retrieval Augmented Language Model Pre-Training</a><a class="anchor-link" href="#Retrieval-Augmented-Language-Model-Pre-Training"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/my_icons/realm.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Augmenting language models with knowledge retrieval sets a new benchmark for open-domain question answering.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I liked this talk a lot because it takes a non-trivial step towards integrating world knowledge into language models and addresses Gary Marcus' <a href="https://thegradient.pub/gpt2-and-the-nature-of-intelligence/">common complaint</a> that data and compute aren't enough to produce Real Intelligence™.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To integrate knowledge into language model pretraining, this talk proposes adding a text retriever that is <em>learned</em> during the training process. Unsurprisingly, this introduces a major computational challenge because the conditional probability now involves a sum over <em>all</em> documents in a corpus $\mathcal{Z}$:</p>
<p>
$$ p(y|x) = \sum_{z\in \mathcal{Z}} p(y|x,z)p(z)\,.$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To deal with this, the authors compute an embedding for every document in the corpus and then use <a href="https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html">Maximum Inner Product Search</a> algorithms to find the approximate top $k$ documents. The result is a hybrid model that significantly outperforms other approaches in open-domain question answering.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Transformers-are-RNNs:-Fast-Autoregressive-Transformers-with-Linear-Attention">
<a class="anchor" href="#Transformers-are-RNNs:-Fast-Autoregressive-Transformers-with-Linear-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2935-Paper.pdf">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a><a class="anchor-link" href="#Transformers-are-RNNs:-Fast-Autoregressive-Transformers-with-Linear-Attention"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/my_icons/transformers-are-rnns.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>A clever choice of kernel reduces the computational complexity of attention from $O(N^2)$ to $O(N)$. Generate images 4000x faster than vanilla transformers <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20">.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's refreshing to see a transformer talk that isn't about using a "bonfire worth of GPU-TPU-neuromorphic wafer scale silicon"<sup id="fnref-4" class="footnote-ref"><a href="#fn-4">4</a></sup> to break NLP benchmarks. This talk observes that the main bottleneck in vanilla transformer models is the softmax attention computation</p>
<p>
$$ V' = \mathrm{softmax} \left(\frac{QK^T}{\sqrt{D}} \right) V $$
</p>
<p>whose time and space complexity is $O(N^2)$ for sequence length $N$. To get around this, the authors first use a similarity function to obtain a <em>generalised</em> form of self-attention</p>
<p>
$$ V_i' = \frac{\sum_j \mathrm{sim}(Q_i, K_j)V_j}{\sum_j \mathrm{sim}(Q_i, K_j)} $$
</p>
<p>which can be simplified via a choice of kernel and matrix associativity:</p>
<p>
$$V_i' = \frac{\phi(Q_i)^T\sum_j\phi(K_j)V_j^T}{\phi(Q_i)^T\sum_j\phi(K_j)}\,. $$
</p>
<p>The result is a self-attention step that is $O(N)$ because the sums in the above expression can be computed once and reused for every query. In practice, this turns out to be especially powerful for inference, with speed-ups of 4000x reported in the talk! The authors go on to show that their formulation can also be used to express transformers as RNNs, which might be an interesting way to explore the <a href="https://mostafadehghani.com/2019/05/05/universal-transformers/">shortcomings</a> of these large langauge models.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="XTREME:-A-Massively-Multilingual-Multi-task-Benchmark-for-Evaluating-Cross-lingual-Generalisation">
<a class="anchor" href="#XTREME:-A-Massively-Multilingual-Multi-task-Benchmark-for-Evaluating-Cross-lingual-Generalisation" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/4220-Paper.pdf">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation</a><a class="anchor-link" href="#XTREME:-A-Massively-Multilingual-Multi-task-Benchmark-for-Evaluating-Cross-lingual-Generalisation"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/my_icons/xtreme.png" alt="" title="Image credit: https://ai.googleblog.com/2020/04/xtreme-massively-multilingual-multi.html"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>A new benchmark to test zero-shot cross-lingual transfer from English to 39 typologically diverse languages.</em></p>
<p>In this talk, the authors introduce the <a href="https://sites.research.google/xtreme">XTREME benchmark</a> to evaluate the ability of multilingual representations to generalise across 40 languages and 9 tasks. To evaluate a model in XTREME, the main idea is to follow a three-stage recipe:</p>
<ol>
<li>Pre-train on a large corpus of multilingual text.</li>
<li>Fine-tune on English data for each task.</li>
<li>Evaluate the model on <em>zero-shot transfer</em> performance, e.g. evaluate the accuracy on a German text classification task.</li>
</ol>
<p>English is chosen for fine-tuning because it's the langauge with the most labelled data, and the authors employ a neat trick using Google Translate to generate proxy test sets for the tasks where a pre-existing translation does not exist.</p>
<p>Although not strictly about Transformers, the baseline models for this benchmark are all variants of the Transformer architecture, and the authors find that <a href="https://arxiv.org/abs/1911.02116">XLM-R</a> achieves the best zero-shot transfer performance across all languages in each task. What I especially like about XTREME is that the tasks are designed to be trainable on a single GPU for less than a day. This should make it possible for research labs with tight budgets to create competitive models, where the gains in performance are likely to come from architectural design rather than simply scaling-up the compute.</p>
<p>I'm excited about this benchmark because I expect it will produce models that have a direct impact on my professional work in Switzerland. With <a href="https://en.wikipedia.org/wiki/Languages_of_Switzerland">four national languages</a> and a smattering of English, building natural language applications that serve the whole population is a constant challenge.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Time-series">
<a class="anchor" href="#Time-series" aria-hidden="true"><span class="octicon octicon-link"></span></a>Time series<a class="anchor-link" href="#Time-series"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Set-Functions-for-Time-Series">
<a class="anchor" href="#Set-Functions-for-Time-Series" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/4750-Paper.pdf">Set Functions for Time Series</a><a class="anchor-link" href="#Set-Functions-for-Time-Series"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/my_icons/seft.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>High-performance classification for irregularly sampled time series</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Time series seems to be the neglected child of machine learning research, so I was happy to see a talk that introduced both theoretical novelty and practical applications.</p>
<p>Main idea is to learn classificaiton models without imputation / interpolation. Introduce SeFT.</p>
<ul>
<li>New approach</li>
<li>Competitive performance with low runtime</li>
<li>Per observation contributions</li>
</ul>
<p>Key ideas: each observation is tuple $(t_i, z_i, m_i)$</p>
<p>Use DeepSets. Find leakage on IP-NETs and Transformers on benchamrk</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Interpretable,-Multidimensional,-Multimodal-Anomaly-Detection-with-Negative-Sampling-for-Detection-of-Device-Failure">
<a class="anchor" href="#Interpretable,-Multidimensional,-Multimodal-Anomaly-Detection-with-Negative-Sampling-for-Detection-of-Device-Failure" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://icml.cc/virtual/2020/poster/6171">Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure</a><a class="anchor-link" href="#Interpretable,-Multidimensional,-Multimodal-Anomaly-Detection-with-Negative-Sampling-for-Detection-of-Device-Failure"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/my_icons/anomaly-detection.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>A new unsupervised anomaly detection for IoT devices.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Fixed rules of supervised approaches no good. Goal $p(x \in \mathrm{normal}) \approx 0$. Uses negative samplign methods. Positive region = observed (most a re normal). Negative region from complement (anomalos). Use integrated gradients to interpret anomaly. Open sourced with madi.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Physics">
<a class="anchor" href="#Physics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Physics<a class="anchor-link" href="#Physics"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Learning-to-Simulate-Complex-Physics-with-Graph-Networks">
<a class="anchor" href="#Learning-to-Simulate-Complex-Physics-with-Graph-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://icml.cc/virtual/2020/poster/6849">Learning to Simulate Complex Physics with Graph Networks</a><a class="anchor-link" href="#Learning-to-Simulate-Complex-Physics-with-Graph-Networks"> </a>
</h3>
<p>
</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/h7h9zF8OO7E" frameborder="0" allowfullscreen=""></iframe>
</center>


</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>A single architecture creates high-fidelity particle simulations of various interacting materials.</em></p>
<p>I'm a sucker for flashy demos and this talk from DeepMind didn't disappoint. They propose an "encode-process-decode" architecture to calculate the dynamics of physical systems, where particle states are represented as graphs and a graph neural network learns the particle interactions.</p>
<p><img src="/blog/images/copied_from_nb/my_icons/gns.png" alt=""></p>
<p>During training, the model predicts each particle's position and velocity one timestep into the future, and these predictions are compared against the ground-truth values of a simulator. Remarkably, this approach generalises to <em>thousands of timesteps</em> at test time, even under different initial conditions and an order of magnitude more particles!<sup id="fnref-3" class="footnote-ref"><a href="#fn-3">3</a></sup></p>
<p>I think this work is a great example of how machine learning can help physicists build better simulations of complex phenomena. It will be interesting to see whether this approach can scale to systems with <em>billions</em> of particles, like those found in <a href="https://wwwmpa.mpa-garching.mpg.de/galform/virgo/millennium/">dark matter simulations</a> or <a href="https://www.youtube.com/watch?v=NhXMXiXOWAA">high-energy collisions</a> at the Large Hadron Collider.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="footnotes"><p id="fn-1">1. Downscaling is needed because naively training on a $224^2 \times 3$ sequence length would blow up the memory of the largest TPU!<a href="#fnref-1" class="footnote footnotes">↩</a></p></div>
<p></p>
<div class="footnotes"><p id="fn-2">2. A <em>linear probe</em> refers to using the model as a feature extractor and passing those features through a linear model like logistic regression.<a href="#fnref-2" class="footnote footnotes">↩</a></p></div>
<p></p>
<div class="footnotes"><p id="fn-3">3. The authors ascribe this generalisation power to the fact that each particle is only aware of local interactions in some 'connectivity radius', so the model is flexible enough to generalise to out-of-distribution inputs.<a href="#fnref-3" class="footnote footnotes">↩</a></p></div>
<p></p>
<div class="footnotes"><p id="fn-4">4. Quote from Stephen Merity's brilliant <em>&lt;a href=<a href="#fnref-4" class="footnote footnotes">↩</a>&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
 

</em></p></div>
</div>
</div></div>
</div>


  </div>
<a class="u-url" href="/blog/research/conference/2020/07/14/icml2020.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Posts on machine learning and physics at irregularly spaced intervals.</p>
      </div>
    </div>

    <div class="social-links">
<ul class="social-media-list">
<li><a rel="me" href="https://github.com/lewtun" title="lewtun"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li>
<li><a rel="me" href="https://twitter.com/_lewtun" title="_lewtun"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
